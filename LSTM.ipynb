{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, TimeDistributed\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "train_y=[]\n",
    "\n",
    "for i in range(1,5001):\n",
    "    train.append(pd.read_csv('./masked_dataset/beat/'+str(i)+'.csv.gzip', compression='gzip').to_numpy())\n",
    "    train_y.append(pd.read_csv('./dataset/beat/'+str(i)+'.csv.gzip', compression='gzip').to_numpy())\n",
    "\n",
    "train = np.array(train)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.swapaxes(1,2)\n",
    "train_y = train_y.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# model.add(Input(shape=(9,32)))\n",
    "\n",
    "# inputs = Input(shape=(9,32))\n",
    "# lstm =  tf.keras.layers.Bidirectional(LSTM(units=32, return_sequences=True, activation='sigmoid'))(inputs)\n",
    "# outputs = tf.keras.layers.Bidirectional(LSTM(units=32, return_sequences=False, activation='sigmoid'))(lstm)\n",
    "# forward = LSTM(units=16, return_sequences=True, activation='sigmoid')\n",
    "# backward = LSTM(units=16, return_sequences=True, activation='sigmoid', go_backwards=True)\n",
    "\n",
    "# model.add(tf.keras.layers.Bidirectional(forward, backward_layer=backward, input_shape=(9,32)))\n",
    "# model.add(LSTM(units=32, return_sequences=True, activation='sigmoid', input_shape=(9,32)))\n",
    "model.add(LSTM(units=9, return_sequences=True, activation='sigmoid', input_shape=(32,9)))\n",
    "# model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = tf.keras.losses.binary_crossentropy, optimizer='rmsprop')\n",
    "model.compile(loss = tf.keras.losses.mse, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train.astype('float32'),train_y.astype('float32'), epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=[]\n",
    "test_y=[]\n",
    "\n",
    "for i in range(10000,10100):\n",
    "    test_x.append(pd.read_csv('./masked_dataset/beat/'+str(i)+'.csv.gzip', compression='gzip').to_numpy())\n",
    "    test_y.append(pd.read_csv('./dataset/beat/'+str(i)+'.csv.gzip', compression='gzip').to_numpy())\n",
    "    \n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.swapaxes(1,2)\n",
    "test_y = test_y.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = np.zeros_like(train)\n",
    "\n",
    "for i in range(dec.shape[0]):\n",
    "    for j in range(dec.shape[1]):\n",
    "        if j != 0:\n",
    "            dec[i][j] = train_y[i][j-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decn = np.zeros_like(test_y)\n",
    "for i in range(decn.shape[0]):\n",
    "    for j in range(decn.shape[1]):\n",
    "        if j != 0:\n",
    "            decn[i][j] = test_y[i][j-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = model.predict(test_x)\n",
    "\n",
    "asdf = asdf.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(test_x[i])\n",
    "print('-'*10)\n",
    "print(asdf[i].astype(int))\n",
    "print('-'*10)\n",
    "print(test_y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(32,9))\n",
    "encoder_lstm = LSTM(units=512, return_state=True, dropout=0.1, recurrent_dropout=0.1)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None,9))\n",
    "decoder_lstm = LSTM(units=512, return_sequences=True, return_state=True, dropout=0.1, recurrent_dropout=0.1)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(9, activation='sigmoid')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model is not None:\n",
    "del model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "125/125 [==============================] - 31s 228ms/step - loss: 0.1013 - val_loss: 0.0951\n",
      "Epoch 2/5000\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0856 - val_loss: 0.0841\n",
      "Epoch 3/5000\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.0774 - val_loss: 0.0793\n",
      "Epoch 4/5000\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 0.0732 - val_loss: 0.0762\n",
      "Epoch 5/5000\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.0705 - val_loss: 0.0738\n",
      "Epoch 6/5000\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.0670 - val_loss: 0.0706\n",
      "Epoch 7/5000\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0639 - val_loss: 0.0665\n",
      "Epoch 8/5000\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0600 - val_loss: 0.0638\n",
      "Epoch 9/5000\n",
      "125/125 [==============================] - 27s 220ms/step - loss: 0.0571 - val_loss: 0.0625\n",
      "Epoch 10/5000\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.0549 - val_loss: 0.0617\n",
      "Epoch 11/5000\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0526 - val_loss: 0.0608\n",
      "Epoch 12/5000\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 0.0508 - val_loss: 0.0597\n",
      "Epoch 13/5000\n",
      "125/125 [==============================] - 27s 220ms/step - loss: 0.0484 - val_loss: 0.0593\n",
      "Epoch 14/5000\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.0465 - val_loss: 0.0585\n",
      "Epoch 15/5000\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.0447 - val_loss: 0.0576\n",
      "Epoch 16/5000\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.0430 - val_loss: 0.0571\n",
      "Epoch 17/5000\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 0.0413 - val_loss: 0.0570\n",
      "Epoch 18/5000\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0397 - val_loss: 0.0558\n",
      "Epoch 19/5000\n",
      "125/125 [==============================] - 29s 230ms/step - loss: 0.0381 - val_loss: 0.0554\n",
      "Epoch 20/5000\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0365 - val_loss: 0.0550\n",
      "Epoch 21/5000\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0353 - val_loss: 0.0554\n",
      "Epoch 22/5000\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0340 - val_loss: 0.0557\n",
      "Epoch 23/5000\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0329 - val_loss: 0.0551\n",
      "Epoch 24/5000\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.0319 - val_loss: 0.0553\n",
      "Epoch 25/5000\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.0308 - val_loss: 0.0559\n",
      "Epoch 26/5000\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.0297 - val_loss: 0.0549\n",
      "Epoch 27/5000\n",
      "125/125 [==============================] - 28s 228ms/step - loss: 0.0287 - val_loss: 0.0549\n",
      "Epoch 28/5000\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0280 - val_loss: 0.0551\n",
      "Epoch 29/5000\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0270 - val_loss: 0.0548\n",
      "Epoch 30/5000\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.0261 - val_loss: 0.0548\n",
      "Epoch 31/5000\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0255 - val_loss: 0.0556\n",
      "Epoch 32/5000\n",
      "125/125 [==============================] - 27s 220ms/step - loss: 0.0246 - val_loss: 0.0552\n",
      "Epoch 33/5000\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0240 - val_loss: 0.0554\n",
      "Epoch 34/5000\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0232 - val_loss: 0.0549\n",
      "Epoch 35/5000\n",
      "125/125 [==============================] - 31s 248ms/step - loss: 0.0225 - val_loss: 0.0563\n",
      "Epoch 36/5000\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 0.0225 - val_loss: 0.0563\n",
      "Epoch 37/5000\n",
      "125/125 [==============================] - 30s 241ms/step - loss: 0.0213 - val_loss: 0.0554\n",
      "Epoch 38/5000\n",
      "125/125 [==============================] - 31s 247ms/step - loss: 0.0206 - val_loss: 0.0554\n",
      "Epoch 39/5000\n",
      "125/125 [==============================] - 31s 244ms/step - loss: 0.0202 - val_loss: 0.0554\n",
      "Epoch 40/5000\n",
      "125/125 [==============================] - 31s 246ms/step - loss: 0.0196 - val_loss: 0.0563\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x=[train, dec], y=train_y, epochs=5000, callbacks=callback, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('LSTM_EnDe_Flip_testloss02806.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 74ms/step - loss: 0.0588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05883701145648956"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=[test_x, decn], y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = model.predict(x=[test_x, decn])\n",
    "asd = asd.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0]]\n",
      "----------\n",
      "[[0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]]\n",
      "----------\n",
      "[[0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(test_x[i])\n",
    "print('-'*10)\n",
    "print(asd[i].astype(int))\n",
    "print('-'*10)\n",
    "print(test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_EnDe_testloss3225.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(512,))\n",
    "decoder_state_input_c = Input(shape=(512,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 9\n",
    "n_timesteps_in = 32\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, n_features))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_seq = list()\n",
    "    while not stop_condition:\n",
    "\n",
    "        # in a loop\n",
    "        # decode the input to a token/output prediction + required states for context vector\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # convert the token/output prediction to a token/output\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_digit = sampled_token_index\n",
    "        # add the predicted token/output to output sequence\n",
    "        decoded_seq.append(sampled_digit)\n",
    "        \n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_digit == '\\n' or\n",
    "           len(decoded_seq) == n_timesteps_in):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the input target sequence (of length 1) \n",
    "        # with the predicted token/output \n",
    "        target_seq = np.zeros((1, 1, n_features))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update input states (context vector) \n",
    "        # with the ouputed states\n",
    "        states_value = [h, c]\n",
    "\n",
    "        # loop back.....\n",
    "        \n",
    "    # when loop exists return the output sequence\n",
    "    return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36975e2464779a1907498622a1ff8d141cf5663f65c7cf0c02ae689c8b788bad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
