{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(1,8001):\n",
    "    tmp = pd.read_csv('./milisec_200_dataset/' + str(i) + '.csv.gzip', compression='gzip').drop('Unnamed: 0', axis=1).to_numpy()\n",
    "    train_x.append(tmp[:199])\n",
    "    train_y.append(tmp[-1])\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 199, 22)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 22)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(LSTM(units=22, activation='softmax', input_shape=(199,22)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer='SGD')\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 3.0342 - val_loss: 2.9786\n",
      "Epoch 2/4000\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 2.9295 - val_loss: 2.8953\n",
      "Epoch 3/4000\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 2.8489 - val_loss: 2.8275\n",
      "Epoch 4/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 2.7813 - val_loss: 2.7676\n",
      "Epoch 5/4000\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 2.7196 - val_loss: 2.7104\n",
      "Epoch 6/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 2.6582 - val_loss: 2.6503\n",
      "Epoch 7/4000\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 2.5906 - val_loss: 2.5806\n",
      "Epoch 8/4000\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 2.5092 - val_loss: 2.4959\n",
      "Epoch 9/4000\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 2.4157 - val_loss: 2.4165\n",
      "Epoch 10/4000\n",
      "200/200 [==============================] - 69s 344ms/step - loss: 2.3161 - val_loss: 2.3606\n",
      "Epoch 11/4000\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 2.2465 - val_loss: 2.3512\n",
      "Epoch 12/4000\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 2.2292 - val_loss: 2.3457\n",
      "Epoch 13/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.2192 - val_loss: 2.3344\n",
      "Epoch 14/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 2.2103 - val_loss: 2.3262\n",
      "Epoch 15/4000\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 2.2015 - val_loss: 2.3197\n",
      "Epoch 16/4000\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 2.1931 - val_loss: 2.3117\n",
      "Epoch 17/4000\n",
      "200/200 [==============================] - 59s 296ms/step - loss: 2.1851 - val_loss: 2.3030\n",
      "Epoch 18/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 2.1773 - val_loss: 2.2922\n",
      "Epoch 19/4000\n",
      "200/200 [==============================] - 58s 288ms/step - loss: 2.1692 - val_loss: 2.2847\n",
      "Epoch 20/4000\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 2.1611 - val_loss: 2.2691\n",
      "Epoch 21/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.1533 - val_loss: 2.2627\n",
      "Epoch 22/4000\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 2.1449 - val_loss: 2.2593\n",
      "Epoch 23/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.1362 - val_loss: 2.2439\n",
      "Epoch 24/4000\n",
      "200/200 [==============================] - 59s 293ms/step - loss: 2.1264 - val_loss: 2.2293\n",
      "Epoch 25/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 2.1155 - val_loss: 2.2168\n",
      "Epoch 26/4000\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 2.1034 - val_loss: 2.2010\n",
      "Epoch 27/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 2.0892 - val_loss: 2.1820\n",
      "Epoch 28/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 2.0748 - val_loss: 2.1646\n",
      "Epoch 29/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 2.0629 - val_loss: 2.1527\n",
      "Epoch 30/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 2.0554 - val_loss: 2.1518\n",
      "Epoch 31/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 2.0514 - val_loss: 2.1457\n",
      "Epoch 32/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 2.0480 - val_loss: 2.1428\n",
      "Epoch 33/4000\n",
      "200/200 [==============================] - 59s 296ms/step - loss: 2.0453 - val_loss: 2.1357\n",
      "Epoch 34/4000\n",
      "200/200 [==============================] - 61s 308ms/step - loss: 2.0424 - val_loss: 2.1343\n",
      "Epoch 35/4000\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 2.0398 - val_loss: 2.1387\n",
      "Epoch 36/4000\n",
      "200/200 [==============================] - 59s 296ms/step - loss: 2.0373 - val_loss: 2.1345\n",
      "Epoch 37/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 2.0348 - val_loss: 2.1249\n",
      "Epoch 38/4000\n",
      "200/200 [==============================] - 59s 295ms/step - loss: 2.0323 - val_loss: 2.1316\n",
      "Epoch 39/4000\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 2.0296 - val_loss: 2.1237\n",
      "Epoch 40/4000\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 2.0267 - val_loss: 2.1197\n",
      "Epoch 41/4000\n",
      "200/200 [==============================] - 64s 322ms/step - loss: 2.0243 - val_loss: 2.1161\n",
      "Epoch 42/4000\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 2.0217 - val_loss: 2.1158\n",
      "Epoch 43/4000\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 2.0189 - val_loss: 2.1138\n",
      "Epoch 44/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 2.0157 - val_loss: 2.1079\n",
      "Epoch 45/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 2.0129 - val_loss: 2.1070\n",
      "Epoch 46/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 2.0096 - val_loss: 2.1002\n",
      "Epoch 47/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 2.0063 - val_loss: 2.1036\n",
      "Epoch 48/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 2.0023 - val_loss: 2.1064\n",
      "Epoch 49/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 1.9983 - val_loss: 2.0874\n",
      "Epoch 50/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9947 - val_loss: 2.0909\n",
      "Epoch 51/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.9904 - val_loss: 2.0854\n",
      "Epoch 52/4000\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.9862 - val_loss: 2.0818\n",
      "Epoch 53/4000\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 1.9813 - val_loss: 2.0696\n",
      "Epoch 54/4000\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 1.9780 - val_loss: 2.0715\n",
      "Epoch 55/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.9740 - val_loss: 2.0644\n",
      "Epoch 56/4000\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 1.9709 - val_loss: 2.0607\n",
      "Epoch 57/4000\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 1.9684 - val_loss: 2.0689\n",
      "Epoch 58/4000\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 1.9661 - val_loss: 2.0584\n",
      "Epoch 59/4000\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 1.9639 - val_loss: 2.0612\n",
      "Epoch 60/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.9620 - val_loss: 2.0569\n",
      "Epoch 61/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9597 - val_loss: 2.0545\n",
      "Epoch 62/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.9582 - val_loss: 2.0482\n",
      "Epoch 63/4000\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 1.9564 - val_loss: 2.0524\n",
      "Epoch 64/4000\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 1.9536 - val_loss: 2.0473\n",
      "Epoch 65/4000\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 1.9510 - val_loss: 2.0422\n",
      "Epoch 66/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.9489 - val_loss: 2.0571\n",
      "Epoch 67/4000\n",
      "200/200 [==============================] - 59s 295ms/step - loss: 1.9464 - val_loss: 2.0355\n",
      "Epoch 68/4000\n",
      "200/200 [==============================] - 59s 295ms/step - loss: 1.9439 - val_loss: 2.0396\n",
      "Epoch 69/4000\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.9407 - val_loss: 2.0340\n",
      "Epoch 70/4000\n",
      "200/200 [==============================] - 62s 313ms/step - loss: 1.9377 - val_loss: 2.0324\n",
      "Epoch 71/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.9342 - val_loss: 2.0270\n",
      "Epoch 72/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9310 - val_loss: 2.0431\n",
      "Epoch 73/4000\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 1.9288 - val_loss: 2.0243\n",
      "Epoch 74/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9274 - val_loss: 2.0125\n",
      "Epoch 75/4000\n",
      "200/200 [==============================] - 61s 305ms/step - loss: 1.9240 - val_loss: 2.0226\n",
      "Epoch 76/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 1.9220 - val_loss: 2.0136\n",
      "Epoch 77/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.9169 - val_loss: 2.0403\n",
      "Epoch 78/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9163 - val_loss: 2.0045\n",
      "Epoch 79/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9138 - val_loss: 2.0057\n",
      "Epoch 80/4000\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 1.9141 - val_loss: 2.0072\n",
      "Epoch 81/4000\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 1.9133 - val_loss: 1.9928\n",
      "Epoch 82/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.9106 - val_loss: 1.9902\n",
      "Epoch 83/4000\n",
      "200/200 [==============================] - 61s 303ms/step - loss: 1.9102 - val_loss: 1.9899\n",
      "Epoch 84/4000\n",
      "200/200 [==============================] - 59s 297ms/step - loss: 1.9080 - val_loss: 1.9873\n",
      "Epoch 85/4000\n",
      "200/200 [==============================] - 61s 303ms/step - loss: 1.9080 - val_loss: 1.9898\n",
      "Epoch 86/4000\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.9062 - val_loss: 1.9841\n",
      "Epoch 87/4000\n",
      "200/200 [==============================] - 67s 336ms/step - loss: 1.9042 - val_loss: 1.9831\n",
      "Epoch 88/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.9047 - val_loss: 1.9825\n",
      "Epoch 89/4000\n",
      "200/200 [==============================] - 67s 336ms/step - loss: 1.9024 - val_loss: 1.9960\n",
      "Epoch 90/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.9000 - val_loss: 1.9784\n",
      "Epoch 91/4000\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 1.9002 - val_loss: 1.9821\n",
      "Epoch 92/4000\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 1.9012 - val_loss: 1.9768\n",
      "Epoch 93/4000\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.8973 - val_loss: 2.0020\n",
      "Epoch 94/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8975 - val_loss: 1.9739\n",
      "Epoch 95/4000\n",
      "200/200 [==============================] - 68s 340ms/step - loss: 1.8964 - val_loss: 1.9844\n",
      "Epoch 96/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8941 - val_loss: 1.9716\n",
      "Epoch 97/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8953 - val_loss: 1.9911\n",
      "Epoch 98/4000\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 1.8955 - val_loss: 1.9800\n",
      "Epoch 99/4000\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.8935 - val_loss: 1.9740\n",
      "Epoch 100/4000\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.8921 - val_loss: 2.0252\n",
      "Epoch 101/4000\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.8923 - val_loss: 1.9697\n",
      "Epoch 102/4000\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 1.8915 - val_loss: 1.9687\n",
      "Epoch 103/4000\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 1.8895 - val_loss: 1.9789\n",
      "Epoch 104/4000\n",
      "200/200 [==============================] - 60s 299ms/step - loss: 1.8892 - val_loss: 1.9657\n",
      "Epoch 105/4000\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 1.8891 - val_loss: 1.9699\n",
      "Epoch 106/4000\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 1.8873 - val_loss: 1.9638\n",
      "Epoch 107/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8882 - val_loss: 1.9618\n",
      "Epoch 108/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.8861 - val_loss: 1.9651\n",
      "Epoch 109/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8843 - val_loss: 1.9609\n",
      "Epoch 110/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8836 - val_loss: 1.9606\n",
      "Epoch 111/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.8844 - val_loss: 1.9742\n",
      "Epoch 112/4000\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 1.8836 - val_loss: 1.9611\n",
      "Epoch 113/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.8835 - val_loss: 1.9585\n",
      "Epoch 114/4000\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 1.8841 - val_loss: 1.9616\n",
      "Epoch 115/4000\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.8820 - val_loss: 1.9588\n",
      "Epoch 116/4000\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 1.8812 - val_loss: 1.9634\n",
      "Epoch 117/4000\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 1.8817 - val_loss: 1.9561\n",
      "Epoch 118/4000\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 1.8781 - val_loss: 1.9761\n",
      "Epoch 119/4000\n",
      "200/200 [==============================] - 64s 322ms/step - loss: 1.8787 - val_loss: 1.9940\n",
      "Epoch 120/4000\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 1.8801 - val_loss: 1.9532\n",
      "Epoch 121/4000\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 1.8779 - val_loss: 1.9523\n",
      "Epoch 122/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8768 - val_loss: 1.9532\n",
      "Epoch 123/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8766 - val_loss: 1.9585\n",
      "Epoch 124/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8759 - val_loss: 1.9519\n",
      "Epoch 125/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8736 - val_loss: 1.9924\n",
      "Epoch 126/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8754 - val_loss: 1.9520\n",
      "Epoch 127/4000\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 1.8740 - val_loss: 1.9692\n",
      "Epoch 128/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.8743 - val_loss: 1.9489\n",
      "Epoch 129/4000\n",
      "200/200 [==============================] - 63s 318ms/step - loss: 1.8703 - val_loss: 1.9563\n",
      "Epoch 130/4000\n",
      "200/200 [==============================] - 65s 323ms/step - loss: 1.8722 - val_loss: 1.9608\n",
      "Epoch 131/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.8702 - val_loss: 1.9615\n",
      "Epoch 132/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8726 - val_loss: 1.9503\n",
      "Epoch 133/4000\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.8701 - val_loss: 1.9470\n",
      "Epoch 134/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8687 - val_loss: 1.9462\n",
      "Epoch 135/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.8669 - val_loss: 1.9437\n",
      "Epoch 136/4000\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 1.8676 - val_loss: 1.9479\n",
      "Epoch 137/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.8676 - val_loss: 1.9424\n",
      "Epoch 138/4000\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 1.8650 - val_loss: 1.9620\n",
      "Epoch 139/4000\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 1.8658 - val_loss: 1.9403\n",
      "Epoch 140/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8638 - val_loss: 1.9413\n",
      "Epoch 141/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8632 - val_loss: 1.9682\n",
      "Epoch 142/4000\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 1.8622 - val_loss: 1.9430\n",
      "Epoch 143/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8623 - val_loss: 1.9380\n",
      "Epoch 144/4000\n",
      "200/200 [==============================] - 67s 337ms/step - loss: 1.8616 - val_loss: 1.9478\n",
      "Epoch 145/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.8611 - val_loss: 1.9384\n",
      "Epoch 146/4000\n",
      "200/200 [==============================] - 64s 322ms/step - loss: 1.8601 - val_loss: 1.9350\n",
      "Epoch 147/4000\n",
      "200/200 [==============================] - 64s 323ms/step - loss: 1.8594 - val_loss: 1.9357\n",
      "Epoch 148/4000\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 1.8588 - val_loss: 1.9441\n",
      "Epoch 149/4000\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 1.8583 - val_loss: 1.9374\n",
      "Epoch 150/4000\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 1.8572 - val_loss: 1.9368\n",
      "Epoch 151/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8567 - val_loss: 1.9325\n",
      "Epoch 152/4000\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 1.8551 - val_loss: 1.9375\n",
      "Epoch 153/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.8549 - val_loss: 1.9315\n",
      "Epoch 154/4000\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 1.8549 - val_loss: 1.9376\n",
      "Epoch 155/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.8531 - val_loss: 1.9296\n",
      "Epoch 156/4000\n",
      "200/200 [==============================] - 65s 323ms/step - loss: 1.8537 - val_loss: 1.9297\n",
      "Epoch 157/4000\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 1.8520 - val_loss: 1.9274\n",
      "Epoch 158/4000\n",
      "200/200 [==============================] - 65s 323ms/step - loss: 1.8510 - val_loss: 1.9301\n",
      "Epoch 159/4000\n",
      "200/200 [==============================] - 60s 298ms/step - loss: 1.8500 - val_loss: 1.9254\n",
      "Epoch 160/4000\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 1.8493 - val_loss: 1.9258\n",
      "Epoch 161/4000\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 1.8485 - val_loss: 1.9231\n",
      "Epoch 162/4000\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 1.8481 - val_loss: 1.9231\n",
      "Epoch 163/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8468 - val_loss: 1.9217\n",
      "Epoch 164/4000\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 1.8470 - val_loss: 1.9240\n",
      "Epoch 165/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.8451 - val_loss: 1.9302\n",
      "Epoch 166/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.8444 - val_loss: 1.9197\n",
      "Epoch 167/4000\n",
      "200/200 [==============================] - 67s 334ms/step - loss: 1.8431 - val_loss: 1.9295\n",
      "Epoch 168/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.8434 - val_loss: 1.9179\n",
      "Epoch 169/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8426 - val_loss: 1.9207\n",
      "Epoch 170/4000\n",
      "200/200 [==============================] - 65s 323ms/step - loss: 1.8406 - val_loss: 1.9159\n",
      "Epoch 171/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8405 - val_loss: 1.9151\n",
      "Epoch 172/4000\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 1.8391 - val_loss: 1.9148\n",
      "Epoch 173/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.8383 - val_loss: 1.9149\n",
      "Epoch 174/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8373 - val_loss: 1.9131\n",
      "Epoch 175/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8368 - val_loss: 1.9221\n",
      "Epoch 176/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.8369 - val_loss: 1.9161\n",
      "Epoch 177/4000\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 1.8357 - val_loss: 1.9100\n",
      "Epoch 178/4000\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 1.8344 - val_loss: 1.9106\n",
      "Epoch 179/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8339 - val_loss: 1.9077\n",
      "Epoch 180/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8327 - val_loss: 1.9064\n",
      "Epoch 181/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8330 - val_loss: 1.9092\n",
      "Epoch 182/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8306 - val_loss: 1.9054\n",
      "Epoch 183/4000\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 1.8307 - val_loss: 1.9037\n",
      "Epoch 184/4000\n",
      "200/200 [==============================] - 65s 323ms/step - loss: 1.8294 - val_loss: 1.9055\n",
      "Epoch 185/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8288 - val_loss: 1.9032\n",
      "Epoch 186/4000\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 1.8277 - val_loss: 1.9022\n",
      "Epoch 187/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.8266 - val_loss: 1.9004\n",
      "Epoch 188/4000\n",
      "200/200 [==============================] - 65s 328ms/step - loss: 1.8259 - val_loss: 1.8992\n",
      "Epoch 189/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8247 - val_loss: 1.8972\n",
      "Epoch 190/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8243 - val_loss: 1.9000\n",
      "Epoch 191/4000\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 1.8231 - val_loss: 1.8948\n",
      "Epoch 192/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8219 - val_loss: 1.8948\n",
      "Epoch 193/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8213 - val_loss: 1.8932\n",
      "Epoch 194/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8208 - val_loss: 1.8918\n",
      "Epoch 195/4000\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 1.8203 - val_loss: 1.8901\n",
      "Epoch 196/4000\n",
      "200/200 [==============================] - 65s 327ms/step - loss: 1.8188 - val_loss: 1.8923\n",
      "Epoch 197/4000\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 1.8177 - val_loss: 1.8881\n",
      "Epoch 198/4000\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 1.8166 - val_loss: 1.8866\n",
      "Epoch 199/4000\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 1.8159 - val_loss: 1.8852\n",
      "Epoch 200/4000\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 1.8141 - val_loss: 1.8836\n",
      "Epoch 201/4000\n",
      "200/200 [==============================] - 65s 328ms/step - loss: 1.8142 - val_loss: 1.8829\n",
      "Epoch 202/4000\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 1.8128 - val_loss: 1.8829\n",
      "Epoch 203/4000\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 1.8117 - val_loss: 1.8842\n",
      "Epoch 204/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.8111 - val_loss: 1.8787\n",
      "Epoch 205/4000\n",
      "200/200 [==============================] - 65s 326ms/step - loss: 1.8106 - val_loss: 1.8745\n",
      "Epoch 206/4000\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 1.8094 - val_loss: 1.8780\n",
      "Epoch 207/4000\n",
      "200/200 [==============================] - 67s 334ms/step - loss: 1.8087 - val_loss: 1.8833\n",
      "Epoch 208/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.8085 - val_loss: 1.8751\n",
      "Epoch 209/4000\n",
      "200/200 [==============================] - 66s 329ms/step - loss: 1.8092 - val_loss: 1.8675\n",
      "Epoch 210/4000\n",
      "200/200 [==============================] - 60s 302ms/step - loss: 1.8101 - val_loss: 1.8751\n",
      "Epoch 211/4000\n",
      "200/200 [==============================] - 60s 300ms/step - loss: 1.8069 - val_loss: 1.8704\n",
      "Epoch 212/4000\n",
      " 39/200 [====>.........................] - ETA: 47s - loss: 1.7874"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\KIST\\Documents\\GitHub\\LSTM_Drummer\\milisec_LSTM.ipynb ì…€ 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KIST/Documents/GitHub/LSTM_Drummer/milisec_LSTM.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/gpu:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/KIST/Documents/GitHub/LSTM_Drummer/milisec_LSTM.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(train_x,train_y, epochs\u001b[39m=\u001b[39;49m\u001b[39m4000\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallback, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1177\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1178\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1179\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1180\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1181\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1182\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1183\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1184\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1185\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3023\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3021\u001b[0m   (graph_function,\n\u001b[0;32m   3022\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3024\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1960\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1956\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1957\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1958\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1959\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1961\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1962\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m     args,\n\u001b[0;32m   1964\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1965\u001b[0m     executing_eagerly)\n\u001b[0;32m   1966\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\KIST\\.conda\\envs\\spleeter\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(train_x,train_y, epochs=4000, callbacks=callback, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(8001,10359):\n",
    "    tmp = pd.read_csv('./milisec_200_dataset/' + str(i) + '.csv.gzip', compression='gzip').drop('Unnamed: 0', axis=1).to_numpy()\n",
    "    test_x.append(tmp[:199])\n",
    "    test_y.append(tmp[-1])\n",
    "\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2358, 199, 22)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd =np.argmax(model.predict(test_x), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = np.argmax(test_y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3469041560644614"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "accuracy_score(prd, act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 14\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 2\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 6\n",
      "-\n",
      "0 14\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "0 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 7\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 12\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 12\n",
      "-\n",
      "1 9\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 0\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 11\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 7\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 11\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 2\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 12\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 6\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 11\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "1 9\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 4\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 8\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 9\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 9\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 6\n",
      "-\n",
      "6 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "1 7\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 11\n",
      "-\n",
      "5 18\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 2\n",
      "-\n",
      "5 4\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 12\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "0 0\n",
      "-\n",
      "5 18\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 2\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "6 12\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 7\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 6\n",
      "-\n",
      "1 8\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 8\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 18\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 8\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "1 11\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 3\n",
      "-\n",
      "0 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "1 3\n",
      "-\n",
      "5 3\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 3\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 18\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 5\n",
      "-\n",
      "6 3\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 9\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 2\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 3\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 3\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 2\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 3\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 5\n",
      "-\n",
      "1 9\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 12\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 11\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 3\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 8\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 11\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 2\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 2\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 13\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 6\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 11\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 7\n",
      "-\n",
      "1 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 0\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 2\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 17\n",
      "-\n",
      "5 2\n",
      "-\n",
      "6 3\n",
      "-\n",
      "6 2\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 12\n",
      "-\n",
      "6 9\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 2\n",
      "-\n",
      "5 3\n",
      "-\n",
      "5 3\n",
      "-\n",
      "5 6\n",
      "-\n",
      "0 11\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 6\n",
      "-\n",
      "6 1\n",
      "-\n",
      "6 5\n",
      "-\n",
      "0 1\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 2\n",
      "-\n",
      "6 0\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 2\n",
      "-\n",
      "6 5\n",
      "-\n",
      "6 12\n",
      "-\n",
      "5 3\n",
      "-\n",
      "1 6\n",
      "-\n",
      "1 2\n",
      "-\n",
      "5 3\n",
      "-\n",
      "5 2\n",
      "-\n",
      "5 2\n",
      "-\n",
      "6 2\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 8\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 8\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 4\n",
      "-\n",
      "1 5\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "1 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 9\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 7\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 12\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 12\n",
      "-\n",
      "6 1\n",
      "-\n",
      "5 0\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 7\n",
      "-\n",
      "1 8\n",
      "-\n",
      "6 5\n",
      "-\n",
      "5 5\n",
      "-\n",
      "6 6\n",
      "-\n",
      "6 6\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 1\n",
      "-\n",
      "5 7\n",
      "-\n",
      "6 0\n",
      "-\n",
      "5 1\n",
      "-\n",
      "1 1\n",
      "-\n",
      "5 4\n",
      "-\n",
      "1 1\n",
      "-\n",
      "6 1\n",
      "-\n",
      "1 0\n",
      "-\n",
      "6 0\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 6\n",
      "-\n",
      "1 0\n",
      "-\n",
      "5 5\n",
      "-\n",
      "5 0\n",
      "-\n",
      "5 7\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(prd[i], act[i])\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 8s 112ms/step - loss: 1.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9955624341964722"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spleeter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b3c401dacfab8a690b1a7de6c6a9edd3269fc10d243267328a900f27334b3a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
